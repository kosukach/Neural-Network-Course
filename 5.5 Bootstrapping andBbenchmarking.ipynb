{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5kOTNEEWzWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit, train_test_split\n",
        "from sklearn import metrics\n",
        "from scipy.stats import zscore\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping \n",
        "import time\n",
        "import statistics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iu6aMpyalFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OQKuCBuXygB",
        "colab_type": "code",
        "outputId": "fd24a909-65a8-4f53-ea5e-f19d9d0263b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
        "    na_values=['NA','?'])\n",
        "df.drop(\"id\", axis=1, inplace=True)\n",
        "\n",
        "# Generate dummies for job\n",
        "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
        "df.drop('job', axis=1, inplace=True)\n",
        "\n",
        "# Generate dummies for area\n",
        "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
        "df.drop('area', axis=1, inplace=True)\n",
        "\n",
        "# Generate dummies for product\n",
        "df = pd.concat([df,pd.get_dummies(df['product'],prefix=\"product\")],axis=1)\n",
        "df.drop('product', axis=1, inplace=True)\n",
        "nonna = df[df[\"income\"].notna()]\n",
        "isna = df[df[\"income\"].isnull()]\n",
        "nx = nonna.drop([\"income\", \"age\"], axis=1).values\n",
        "ny = nonna[\"income\"].values\n",
        "nx_train, nx_test, ny_train, ny_test = train_test_split(nx, ny, test_size=0.2)\n",
        "reg = LinearRegression().fit(nx_train, ny_train)\n",
        "isna[\"income\"] = reg.predict(isna.drop([\"income\", \"age\"], axis=1).values)\n",
        "df = pd.concat([isna, nonna], axis=0)\n",
        "reg.score(nx_test, ny_test)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8454637982792446"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhIaGF6FYUm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['income'] = zscore(df['income'])\n",
        "df['aspect'] = zscore(df['aspect'])\n",
        "df['save_rate'] = zscore(df['save_rate'])\n",
        "df['subscriptions'] = zscore(df['subscriptions'])\n",
        "df[\"dist_unhealthy\"] = zscore(df[\"dist_unhealthy\"])\n",
        "df[\"dist_healthy\"] = zscore(df[\"dist_healthy\"])\n",
        "x_columns = df.columns.drop('age')\n",
        "x = df[x_columns].values\n",
        "y = df['age'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-pZAZbhYfMJ",
        "colab_type": "code",
        "outputId": "f4487844-55ba-4938-b174-065f5ea13375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>income</th>\n",
              "      <th>aspect</th>\n",
              "      <th>subscriptions</th>\n",
              "      <th>dist_healthy</th>\n",
              "      <th>save_rate</th>\n",
              "      <th>dist_unhealthy</th>\n",
              "      <th>age</th>\n",
              "      <th>pop_dense</th>\n",
              "      <th>retail_dense</th>\n",
              "      <th>crime</th>\n",
              "      <th>job_11</th>\n",
              "      <th>job_al</th>\n",
              "      <th>job_am</th>\n",
              "      <th>job_ax</th>\n",
              "      <th>job_bf</th>\n",
              "      <th>job_by</th>\n",
              "      <th>job_cv</th>\n",
              "      <th>job_de</th>\n",
              "      <th>job_dz</th>\n",
              "      <th>job_e2</th>\n",
              "      <th>job_f8</th>\n",
              "      <th>job_gj</th>\n",
              "      <th>job_gv</th>\n",
              "      <th>job_kd</th>\n",
              "      <th>job_ke</th>\n",
              "      <th>job_kl</th>\n",
              "      <th>job_kp</th>\n",
              "      <th>job_ks</th>\n",
              "      <th>job_kw</th>\n",
              "      <th>job_mm</th>\n",
              "      <th>job_nb</th>\n",
              "      <th>job_nn</th>\n",
              "      <th>job_ob</th>\n",
              "      <th>job_pe</th>\n",
              "      <th>job_po</th>\n",
              "      <th>job_pq</th>\n",
              "      <th>job_pz</th>\n",
              "      <th>job_qp</th>\n",
              "      <th>job_qw</th>\n",
              "      <th>job_rn</th>\n",
              "      <th>job_sa</th>\n",
              "      <th>job_vv</th>\n",
              "      <th>job_zz</th>\n",
              "      <th>area_a</th>\n",
              "      <th>area_b</th>\n",
              "      <th>area_c</th>\n",
              "      <th>area_d</th>\n",
              "      <th>product_a</th>\n",
              "      <th>product_b</th>\n",
              "      <th>product_c</th>\n",
              "      <th>product_d</th>\n",
              "      <th>product_e</th>\n",
              "      <th>product_f</th>\n",
              "      <th>product_g</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>-0.150065</td>\n",
              "      <td>-0.494600</td>\n",
              "      <td>-0.208449</td>\n",
              "      <td>-0.670247</td>\n",
              "      <td>-0.387695</td>\n",
              "      <td>-1.105761</td>\n",
              "      <td>50</td>\n",
              "      <td>0.874016</td>\n",
              "      <td>0.417323</td>\n",
              "      <td>0.238394</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>0.245186</td>\n",
              "      <td>-0.996591</td>\n",
              "      <td>-0.208449</td>\n",
              "      <td>-0.000942</td>\n",
              "      <td>-0.542432</td>\n",
              "      <td>-1.169511</td>\n",
              "      <td>47</td>\n",
              "      <td>0.858268</td>\n",
              "      <td>0.503937</td>\n",
              "      <td>0.263349</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.848228</td>\n",
              "      <td>1.360973</td>\n",
              "      <td>-1.255928</td>\n",
              "      <td>1.294946</td>\n",
              "      <td>-0.353308</td>\n",
              "      <td>-0.310185</td>\n",
              "      <td>44</td>\n",
              "      <td>0.925197</td>\n",
              "      <td>0.692913</td>\n",
              "      <td>0.086156</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>0.130981</td>\n",
              "      <td>-1.408940</td>\n",
              "      <td>-0.208449</td>\n",
              "      <td>-0.247778</td>\n",
              "      <td>0.282834</td>\n",
              "      <td>-0.487775</td>\n",
              "      <td>44</td>\n",
              "      <td>0.826772</td>\n",
              "      <td>0.586614</td>\n",
              "      <td>0.303499</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>0.424151</td>\n",
              "      <td>0.724521</td>\n",
              "      <td>-0.208449</td>\n",
              "      <td>0.644628</td>\n",
              "      <td>0.661081</td>\n",
              "      <td>2.321132</td>\n",
              "      <td>43</td>\n",
              "      <td>0.992126</td>\n",
              "      <td>0.775591</td>\n",
              "      <td>0.443469</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       income    aspect  subscriptions  ...  product_e  product_f  product_g\n",
              "35  -0.150065 -0.494600      -0.208449  ...          0          0          0\n",
              "69   0.245186 -0.996591      -0.208449  ...          0          0          0\n",
              "101  0.848228  1.360973      -1.255928  ...          0          0          0\n",
              "134  0.130981 -1.408940      -0.208449  ...          0          0          0\n",
              "155  0.424151  0.724521      -0.208449  ...          0          0          0\n",
              "\n",
              "[5 rows x 54 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXCXuBoHYhBo",
        "colab_type": "code",
        "outputId": "3062ebd3-9e9f-47b4-cdaf-b993608c0b6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "SPLITS = 50\n",
        "\n",
        "boot = ShuffleSplit(n_splits=SPLITS, test_size=0.1, random_state=69)\n",
        "\n",
        "mean_benchmark = []\n",
        "epochs_needed = []\n",
        "num = 0\n",
        "\n",
        "for train, test in boot.split(x):\n",
        "  num += 1\n",
        "  print(f\"Cycle #{num}\\n\")\n",
        "\n",
        "  x_train = x[train]\n",
        "  x_test = x[test]\n",
        "  y_train = y[train]\n",
        "  y_test = y[test]\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(50, input_dim=x.shape[1], activation=\"relu\", ))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(25, activation=\"relu\"))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
        "  \n",
        "  monitor = EarlyStopping(monitor=\"val_loss\", min_delta=1e-3, patience=10,\n",
        "                          verbose=0, mode=\"auto\", restore_best_weights=True)\n",
        "  \n",
        "  start_time = time.time()\n",
        "  model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
        "           callbacks=[monitor], verbose=0, epochs=500)\n",
        "  time_took = time.time() - start_time\n",
        "  \n",
        "  epochs = monitor.stopped_epoch\n",
        "  epochs_needed.append(epochs)\n",
        " \n",
        "  pred = model.predict(x_test)\n",
        "\n",
        "  score = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
        "  mean_benchmark.append(score)\n",
        "\n",
        "  m1 = statistics.mean(mean_benchmark)\n",
        "  m2 = statistics.mean(epochs_needed)\n",
        "  mdev = statistics.pstdev(mean_benchmark)\n",
        "  print(f\"score={score:.6f}, mean score={m1:.6f}, stdev={mdev:.6f}, epochs={epochs}, mean epochs={int(m2)}, time={hms_string(time_took)}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cycle #1\n",
            "\n",
            "score=1.037836, mean score=1.037836, stdev=0.000000, epochs=89, mean epochs=89, time=0:00:08.14\n",
            "Cycle #2\n",
            "\n",
            "score=1.193826, mean score=1.115831, stdev=0.077995, epochs=88, mean epochs=88, time=0:00:07.96\n",
            "Cycle #3\n",
            "\n",
            "score=1.331748, mean score=1.187803, stdev=0.120065, epochs=98, mean epochs=91, time=0:00:08.77\n",
            "Cycle #4\n",
            "\n",
            "score=1.484980, mean score=1.262098, stdev=0.165440, epochs=102, mean epochs=94, time=0:00:08.96\n",
            "Cycle #5\n",
            "\n",
            "score=1.234768, mean score=1.256632, stdev=0.148378, epochs=102, mean epochs=95, time=0:00:09.03\n",
            "Cycle #6\n",
            "\n",
            "score=1.449047, mean score=1.288701, stdev=0.153261, epochs=96, mean epochs=95, time=0:00:08.87\n",
            "Cycle #7\n",
            "\n",
            "score=1.118257, mean score=1.264352, stdev=0.153917, epochs=79, mean epochs=93, time=0:00:07.21\n",
            "Cycle #8\n",
            "\n",
            "score=1.201830, mean score=1.256537, stdev=0.145454, epochs=106, mean epochs=95, time=0:00:09.34\n",
            "Cycle #9\n",
            "\n",
            "score=1.512303, mean score=1.284955, stdev=0.158956, epochs=70, mean epochs=92, time=0:00:06.24\n",
            "Cycle #10\n",
            "\n",
            "score=1.100035, mean score=1.266463, stdev=0.160679, epochs=102, mean epochs=93, time=0:00:09.98\n",
            "Cycle #11\n",
            "\n",
            "score=1.358813, mean score=1.274859, stdev=0.155485, epochs=96, mean epochs=93, time=0:00:08.60\n",
            "Cycle #12\n",
            "\n",
            "score=1.203067, mean score=1.268876, stdev=0.150182, epochs=91, mean epochs=93, time=0:00:08.10\n",
            "Cycle #13\n",
            "\n",
            "score=1.298573, mean score=1.271160, stdev=0.144507, epochs=71, mean epochs=91, time=0:00:06.50\n",
            "Cycle #14\n",
            "\n",
            "score=1.251901, mean score=1.269785, stdev=0.139339, epochs=108, mean epochs=92, time=0:00:09.75\n",
            "Cycle #15\n",
            "\n",
            "score=1.196776, mean score=1.264917, stdev=0.135840, epochs=83, mean epochs=92, time=0:00:07.29\n",
            "Cycle #16\n",
            "\n",
            "score=1.099833, mean score=1.254600, stdev=0.137463, epochs=100, mean epochs=92, time=0:00:08.97\n",
            "Cycle #17\n",
            "\n",
            "score=1.123054, mean score=1.246862, stdev=0.136904, epochs=110, mean epochs=93, time=0:00:09.70\n",
            "Cycle #18\n",
            "\n",
            "score=1.950927, mean score=1.285976, stdev=0.209071, epochs=54, mean epochs=91, time=0:00:05.18\n",
            "Cycle #19\n",
            "\n",
            "score=1.424091, mean score=1.293246, stdev=0.205819, epochs=87, mean epochs=91, time=0:00:07.65\n",
            "Cycle #20\n",
            "\n",
            "score=1.577001, mean score=1.307433, stdev=0.209923, epochs=71, mean epochs=90, time=0:00:06.47\n",
            "Cycle #21\n",
            "\n",
            "score=1.003157, mean score=1.292944, stdev=0.214868, epochs=102, mean epochs=90, time=0:00:09.28\n",
            "Cycle #22\n",
            "\n",
            "score=1.055617, mean score=1.282156, stdev=0.215670, epochs=91, mean epochs=90, time=0:00:08.16\n",
            "Cycle #23\n",
            "\n",
            "score=1.059597, mean score=1.272480, stdev=0.215757, epochs=91, mean epochs=90, time=0:00:08.14\n",
            "Cycle #24\n",
            "\n",
            "score=1.055278, mean score=1.263430, stdev=0.215628, epochs=100, mean epochs=91, time=0:00:08.71\n",
            "Cycle #25\n",
            "\n",
            "score=1.098615, mean score=1.256837, stdev=0.213725, epochs=84, mean epochs=90, time=0:00:07.60\n",
            "Cycle #26\n",
            "\n",
            "score=2.328562, mean score=1.298057, stdev=0.293937, epochs=53, mean epochs=89, time=0:00:04.91\n",
            "Cycle #27\n",
            "\n",
            "score=1.207254, mean score=1.294694, stdev=0.288952, epochs=101, mean epochs=89, time=0:00:09.12\n",
            "Cycle #28\n",
            "\n",
            "score=1.183041, mean score=1.290707, stdev=0.284501, epochs=88, mean epochs=89, time=0:00:07.80\n",
            "Cycle #29\n",
            "\n",
            "score=1.175513, mean score=1.286735, stdev=0.280342, epochs=104, mean epochs=90, time=0:00:09.26\n",
            "Cycle #30\n",
            "\n",
            "score=1.584925, mean score=1.296674, stdev=0.280779, epochs=74, mean epochs=89, time=0:00:06.43\n",
            "Cycle #31\n",
            "\n",
            "score=1.100017, mean score=1.290330, stdev=0.278390, epochs=102, mean epochs=90, time=0:00:09.11\n",
            "Cycle #32\n",
            "\n",
            "score=1.117024, mean score=1.284915, stdev=0.275660, epochs=93, mean epochs=90, time=0:00:08.25\n",
            "Cycle #33\n",
            "\n",
            "score=1.061491, mean score=1.278144, stdev=0.274140, epochs=101, mean epochs=90, time=0:00:09.02\n",
            "Cycle #34\n",
            "\n",
            "score=1.249931, mean score=1.277314, stdev=0.270120, epochs=101, mean epochs=90, time=0:00:08.99\n",
            "Cycle #35\n",
            "\n",
            "score=1.204117, mean score=1.275223, stdev=0.266513, epochs=78, mean epochs=90, time=0:00:07.18\n",
            "Cycle #36\n",
            "\n",
            "score=1.156798, mean score=1.271933, stdev=0.263505, epochs=92, mean epochs=90, time=0:00:08.25\n",
            "Cycle #37\n",
            "\n",
            "score=1.428442, mean score=1.276163, stdev=0.261155, epochs=95, mean epochs=90, time=0:00:08.51\n",
            "Cycle #38\n",
            "\n",
            "score=1.118458, mean score=1.272013, stdev=0.258930, epochs=87, mean epochs=90, time=0:00:07.81\n",
            "Cycle #39\n",
            "\n",
            "score=1.161958, mean score=1.269191, stdev=0.256180, epochs=103, mean epochs=90, time=0:00:09.20\n",
            "Cycle #40\n",
            "\n",
            "score=0.959342, mean score=1.261445, stdev=0.257541, epochs=107, mean epochs=91, time=0:00:09.70\n",
            "Cycle #41\n",
            "\n",
            "score=1.133316, mean score=1.258320, stdev=0.255148, epochs=100, mean epochs=91, time=0:00:08.93\n",
            "Cycle #42\n",
            "\n",
            "score=1.222951, mean score=1.257478, stdev=0.252150, epochs=95, mean epochs=91, time=0:00:08.43\n",
            "Cycle #43\n",
            "\n",
            "score=1.451096, mean score=1.261981, stdev=0.250903, epochs=84, mean epochs=91, time=0:00:07.43\n",
            "Cycle #44\n",
            "\n",
            "score=1.034976, mean score=1.256821, stdev=0.250332, epochs=101, mean epochs=91, time=0:00:09.36\n",
            "Cycle #45\n",
            "\n",
            "score=1.175320, mean score=1.255010, stdev=0.247827, epochs=101, mean epochs=91, time=0:00:09.03\n",
            "Cycle #46\n",
            "\n",
            "score=1.066612, mean score=1.250915, stdev=0.246653, epochs=90, mean epochs=91, time=0:00:07.92\n",
            "Cycle #47\n",
            "\n",
            "score=1.053172, mean score=1.246707, stdev=0.245678, epochs=84, mean epochs=91, time=0:00:07.91\n",
            "Cycle #48\n",
            "\n",
            "score=1.130162, mean score=1.244279, stdev=0.243674, epochs=99, mean epochs=91, time=0:00:09.41\n",
            "Cycle #49\n",
            "\n",
            "score=1.118709, mean score=1.241717, stdev=0.241828, epochs=81, mean epochs=91, time=0:00:07.14\n",
            "Cycle #50\n",
            "\n",
            "score=1.055682, mean score=1.237996, stdev=0.240810, epochs=96, mean epochs=91, time=0:00:08.41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opi-K3VwJAyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "385a067c-c20f-4311-a1de-283aa2b80314"
      },
      "source": [
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
        "    na_values=['NA','?'])\n",
        "# Generate dummies for job\n",
        "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
        "df.drop('job', axis=1, inplace=True)\n",
        "\n",
        "# Generate dummies for area\n",
        "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
        "df.drop('area', axis=1, inplace=True)\n",
        "nonna = df[df[\"income\"].notna()]\n",
        "isna = df[df[\"income\"].isnull()]\n",
        "nx = nonna.drop([\"income\", \"product\"], axis=1).values\n",
        "ny = nonna[\"income\"].values\n",
        "nx_train, nx_test, ny_train, ny_test = train_test_split(nx, ny, test_size=0.2)\n",
        "reg = LinearRegression().fit(nx_train, ny_train)\n",
        "isna[\"income\"] = reg.predict(isna.drop([\"income\", \"product\"], axis=1).values)\n",
        "df = pd.concat([isna, nonna], axis=0)\n",
        "reg.score(nx_test, ny_test)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8128780309405559"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvxONK0OJ3xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['income'] = zscore(df['income'])\n",
        "df['aspect'] = zscore(df['aspect'])\n",
        "df['save_rate'] = zscore(df['save_rate'])\n",
        "df['subscriptions'] = zscore(df['subscriptions'])\n",
        "df[\"dist_unhealthy\"] = zscore(df[\"dist_unhealthy\"])\n",
        "\n",
        "x_col = df.columns.drop(\"product\").drop(\"id\")\n",
        "x = df[x_col].values\n",
        "\n",
        "dummies = pd.get_dummies(df['product']) # Classification\n",
        "products = dummies.columns\n",
        "y = dummies.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBgNEjFgJ4tV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5512c905-2aa6-488f-b32f-35d66df4aeb5"
      },
      "source": [
        "Splits = 50\n",
        "\n",
        "boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1, random_state=69)\n",
        "\n",
        "mean_benchmark = []\n",
        "epochs_needed = []\n",
        "cycle = 0\n",
        "\n",
        "for train, test in boot.split(x, df[\"product\"]):\n",
        "  cycle += 1\n",
        "  print(f\"Cycle #{cycle}\")\n",
        "\n",
        "  x_train = x[train]\n",
        "  x_test = x[test]\n",
        "  y_train = y[train]\n",
        "  y_test = y[test]\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(50, input_dim=x.shape[1], activation=\"relu\"))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(25, activation=\"relu\"))\n",
        "  model.add(Dense(y.shape[1], activation=\"softmax\"))\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "  monitor = EarlyStopping(monitor=\"val_loss\", min_delta=1e-3, patience=10,\n",
        "                          verbose=0, mode=\"auto\", restore_best_weights=True)\n",
        "\n",
        "  start_time = time.time()\n",
        "  model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
        "            epochs=500, verbose=0, callbacks=[monitor])\n",
        "  time_took = time.time() - start_time\n",
        "\n",
        "  epochs = monitor.stopped_epoch\n",
        "\n",
        "  pred = model.predict(x_test)\n",
        "  y_compare = np.argmax(y_test, axis=1)\n",
        "  score = metrics.log_loss(y_compare, pred)\n",
        "\n",
        "  epochs_needed.append(epochs)\n",
        "  mean_benchmark.append(score)\n",
        "\n",
        "  m1 = statistics.mean(mean_benchmark)\n",
        "  m2 = statistics.mean(epochs_needed)\n",
        "  mdev = statistics.pstdev(mean_benchmark)\n",
        "\n",
        "  print(f\"log_loss={score:.6f}, mean log_loss={m1:.6f}, stdev={mdev:.6f}, epochs={epochs}, mean epochs={int(m2)}, time={hms_string(time_took)}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cycle #1\n",
            "log_loss=0.712854, mean log_loss=0.712854, stdev=0.000000, epochs=67, mean epochs=67, time=0:00:06.27\n",
            "Cycle #2\n",
            "log_loss=0.712304, mean log_loss=0.712579, stdev=0.000275, epochs=65, mean epochs=66, time=0:00:06.07\n",
            "Cycle #3\n",
            "log_loss=0.704724, mean log_loss=0.709961, stdev=0.003710, epochs=71, mean epochs=67, time=0:00:06.67\n",
            "Cycle #4\n",
            "log_loss=0.724125, mean log_loss=0.713502, stdev=0.006924, epochs=62, mean epochs=66, time=0:00:05.74\n",
            "Cycle #5\n",
            "log_loss=0.719642, mean log_loss=0.714730, stdev=0.006662, epochs=58, mean epochs=64, time=0:00:05.42\n",
            "Cycle #6\n",
            "log_loss=0.683200, mean log_loss=0.709475, stdev=0.013231, epochs=158, mean epochs=80, time=0:00:14.24\n",
            "Cycle #7\n",
            "log_loss=0.746456, mean log_loss=0.714758, stdev=0.017819, epochs=101, mean epochs=83, time=0:00:09.25\n",
            "Cycle #8\n",
            "log_loss=0.711388, mean log_loss=0.714337, stdev=0.016705, epochs=84, mean epochs=83, time=0:00:07.75\n",
            "Cycle #9\n",
            "log_loss=0.756264, mean log_loss=0.718995, stdev=0.020535, epochs=63, mean epochs=81, time=0:00:06.19\n",
            "Cycle #10\n",
            "log_loss=0.666442, mean log_loss=0.713740, stdev=0.025061, epochs=93, mean epochs=82, time=0:00:08.43\n",
            "Cycle #11\n",
            "log_loss=0.649611, mean log_loss=0.707910, stdev=0.030180, epochs=78, mean epochs=81, time=0:00:07.57\n",
            "Cycle #12\n",
            "log_loss=0.692898, mean log_loss=0.706659, stdev=0.029192, epochs=80, mean epochs=81, time=0:00:07.53\n",
            "Cycle #13\n",
            "log_loss=0.613891, mean log_loss=0.699523, stdev=0.037385, epochs=78, mean epochs=81, time=0:00:07.12\n",
            "Cycle #14\n",
            "log_loss=0.659530, mean log_loss=0.696666, stdev=0.037469, epochs=109, mean epochs=83, time=0:00:09.83\n",
            "Cycle #15\n",
            "log_loss=0.759098, mean log_loss=0.700828, stdev=0.039406, epochs=52, mean epochs=81, time=0:00:04.94\n",
            "Cycle #16\n",
            "log_loss=0.605123, mean log_loss=0.694847, stdev=0.044637, epochs=94, mean epochs=82, time=0:00:08.79\n",
            "Cycle #17\n",
            "log_loss=0.664793, mean log_loss=0.693079, stdev=0.043878, epochs=69, mean epochs=81, time=0:00:06.55\n",
            "Cycle #18\n",
            "log_loss=0.686244, mean log_loss=0.692699, stdev=0.042671, epochs=93, mean epochs=81, time=0:00:08.50\n",
            "Cycle #19\n",
            "log_loss=0.683722, mean log_loss=0.692227, stdev=0.041581, epochs=82, mean epochs=81, time=0:00:07.45\n",
            "Cycle #20\n",
            "log_loss=0.770092, mean log_loss=0.696120, stdev=0.043938, epochs=75, mean epochs=81, time=0:00:07.04\n",
            "Cycle #21\n",
            "log_loss=0.705440, mean log_loss=0.696564, stdev=0.042925, epochs=84, mean epochs=81, time=0:00:07.70\n",
            "Cycle #22\n",
            "log_loss=0.701397, mean log_loss=0.696784, stdev=0.041950, epochs=120, mean epochs=83, time=0:00:11.15\n",
            "Cycle #23\n",
            "log_loss=0.757385, mean log_loss=0.699418, stdev=0.042849, epochs=66, mean epochs=82, time=0:00:06.52\n",
            "Cycle #24\n",
            "log_loss=0.704447, mean log_loss=0.699628, stdev=0.041959, epochs=67, mean epochs=82, time=0:00:06.51\n",
            "Cycle #25\n",
            "log_loss=0.636270, mean log_loss=0.697094, stdev=0.042945, epochs=78, mean epochs=81, time=0:00:07.20\n",
            "Cycle #26\n",
            "log_loss=0.737128, mean log_loss=0.698633, stdev=0.042809, epochs=58, mean epochs=80, time=0:00:05.43\n",
            "Cycle #27\n",
            "log_loss=0.756940, mean log_loss=0.700793, stdev=0.043428, epochs=53, mean epochs=79, time=0:00:05.26\n",
            "Cycle #28\n",
            "log_loss=0.682614, mean log_loss=0.700144, stdev=0.042778, epochs=115, mean epochs=81, time=0:00:10.18\n",
            "Cycle #29\n",
            "log_loss=0.689944, mean log_loss=0.699792, stdev=0.042076, epochs=100, mean epochs=81, time=0:00:09.12\n",
            "Cycle #30\n",
            "log_loss=0.685737, mean log_loss=0.699323, stdev=0.041445, epochs=61, mean epochs=81, time=0:00:05.80\n",
            "Cycle #31\n",
            "log_loss=0.669282, mean log_loss=0.698354, stdev=0.041115, epochs=96, mean epochs=81, time=0:00:09.00\n",
            "Cycle #32\n",
            "log_loss=0.717943, mean log_loss=0.698967, stdev=0.040611, epochs=74, mean epochs=81, time=0:00:06.87\n",
            "Cycle #33\n",
            "log_loss=0.746227, mean log_loss=0.700399, stdev=0.040803, epochs=65, mean epochs=80, time=0:00:06.44\n",
            "Cycle #34\n",
            "log_loss=0.637261, mean log_loss=0.698542, stdev=0.041590, epochs=97, mean epochs=81, time=0:00:08.81\n",
            "Cycle #35\n",
            "log_loss=0.718344, mean log_loss=0.699107, stdev=0.041124, epochs=87, mean epochs=81, time=0:00:07.86\n",
            "Cycle #36\n",
            "log_loss=0.667312, mean log_loss=0.698224, stdev=0.040884, epochs=106, mean epochs=82, time=0:00:09.67\n",
            "Cycle #37\n",
            "log_loss=0.698875, mean log_loss=0.698242, stdev=0.040328, epochs=69, mean epochs=81, time=0:00:06.54\n",
            "Cycle #38\n",
            "log_loss=0.706472, mean log_loss=0.698458, stdev=0.039816, epochs=88, mean epochs=82, time=0:00:08.36\n",
            "Cycle #39\n",
            "log_loss=0.671767, mean log_loss=0.697774, stdev=0.039528, epochs=96, mean epochs=82, time=0:00:08.74\n",
            "Cycle #40\n",
            "log_loss=0.773439, mean log_loss=0.699666, stdev=0.040779, epochs=48, mean epochs=81, time=0:00:04.60\n",
            "Cycle #41\n",
            "log_loss=0.690294, mean log_loss=0.699437, stdev=0.040305, epochs=75, mean epochs=81, time=0:00:06.92\n",
            "Cycle #42\n",
            "log_loss=0.675584, mean log_loss=0.698869, stdev=0.039988, epochs=60, mean epochs=80, time=0:00:05.63\n",
            "Cycle #43\n",
            "log_loss=0.664128, mean log_loss=0.698061, stdev=0.039865, epochs=87, mean epochs=80, time=0:00:07.84\n",
            "Cycle #44\n",
            "log_loss=0.659860, mean log_loss=0.697193, stdev=0.039819, epochs=70, mean epochs=80, time=0:00:06.77\n",
            "Cycle #45\n",
            "log_loss=0.676957, mean log_loss=0.696743, stdev=0.039487, epochs=72, mean epochs=80, time=0:00:06.75\n",
            "Cycle #46\n",
            "log_loss=0.714399, mean log_loss=0.697127, stdev=0.039140, epochs=62, mean epochs=80, time=0:00:05.83\n",
            "Cycle #47\n",
            "log_loss=0.689590, mean log_loss=0.696967, stdev=0.038737, epochs=69, mean epochs=79, time=0:00:06.40\n",
            "Cycle #48\n",
            "log_loss=0.706168, mean log_loss=0.697158, stdev=0.038354, epochs=109, mean epochs=80, time=0:00:09.82\n",
            "Cycle #49\n",
            "log_loss=0.671834, mean log_loss=0.696642, stdev=0.038129, epochs=67, mean epochs=80, time=0:00:06.30\n",
            "Cycle #50\n",
            "log_loss=0.759754, mean log_loss=0.697904, stdev=0.038766, epochs=73, mean epochs=80, time=0:00:07.02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnTKd2h7L4PA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce18177c-7360-41e4-c29c-bb203f6d5b07"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjUFc8yRL9kX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02a59f35-6774-45b4-a9b1-d3fce98f41a3"
      },
      "source": [
        "#####OPTIMAL########\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow.keras.initializers\n",
        "import statistics\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from tensorflow.keras.layers import LeakyReLU,PReLU\n",
        "\n",
        "SPLITS = 100\n",
        "\n",
        "# Bootstrap\n",
        "boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1)\n",
        "\n",
        "# Track progress\n",
        "mean_benchmark = []\n",
        "epochs_needed = []\n",
        "num = 0\n",
        "\n",
        "# Loop through samples\n",
        "for train, test in boot.split(x,df['product']):\n",
        "    start_time = time.time()\n",
        "    num+=1\n",
        "\n",
        "    # Split train and test\n",
        "    x_train = x[train]\n",
        "    y_train = y[train]\n",
        "    x_test = x[test]\n",
        "    y_test = y[test]\n",
        "\n",
        "    # Construct neural network\n",
        "    # kernel_initializer = tensorflow.keras.initializers.he_uniform(seed=None)\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=x.shape[1], activation=PReLU(), kernel_regularizer=regularizers.l2(1e-4)\n",
        "    )) # Hidden 1\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100, activation=PReLU(), activity_regularizer=regularizers.l2(1e-4)\n",
        "    )) # Hidden 2\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100, activation=PReLU(), activity_regularizer=regularizers.l2(1e-4)\n",
        "    )) # Hidden 3\n",
        "#    model.add(Dropout(0.5)) - Usually better performance without dropout on final layer\n",
        "    model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
        "        patience=100, verbose=0, mode='auto', restore_best_weights=True)\n",
        "\n",
        "    # Train on the bootstrap sample\n",
        "    model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n",
        "    epochs = monitor.stopped_epoch\n",
        "    epochs_needed.append(epochs)\n",
        "    \n",
        "    # Predict on the out of boot (validation)\n",
        "    pred = model.predict(x_test)\n",
        "  \n",
        "    # Measure this bootstrap's log loss\n",
        "    y_compare = np.argmax(y_test,axis=1) # For log loss calculation\n",
        "    score = metrics.log_loss(y_compare, pred)\n",
        "    mean_benchmark.append(score)\n",
        "    m1 = statistics.mean(mean_benchmark)\n",
        "    m2 = statistics.mean(epochs_needed)\n",
        "    mdev = statistics.pstdev(mean_benchmark)\n",
        "    \n",
        "    # Record this iteration\n",
        "    time_took = time.time() - start_time\n",
        "    print(f\"#{num}: score={score:.6f}, mean score={m1:.6f}, stdev={mdev:.6f}, epochs={epochs}, mean epochs={int(m2)}, time={hms_string(time_took)}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.69716"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPg3G7iqQkOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}